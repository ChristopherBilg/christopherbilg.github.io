<!DOCTYPE html>
<html lang="en" color-mode="user">
  <head>
    <meta charset="UTF-8" />
    <meta name="description" content="Christopher Bilger's Portfolio" />
    <meta name="keywords" content="Resume, Portfolio, Personal, Projects, Presentations" />
    <meta name="author" content="Christopher Bilger" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <meta property="og:url" content="https://chrisbilger.com" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Chris Bilger's Portfolio" />
    <meta property="og:description" content="Christopher Bilger's Portfolio" />
    <meta property="og:image" content="https://chrisbilger.com/images/portfolio.avif" />

    <title>Assessment State Machine</title>
    <link rel="icon" type="image/x-icon" href="../../images/favicon.ico" />

    <link rel="stylesheet" href="../lib/reveal.js-5.1.0/reset.css" />
    <link rel="stylesheet" href="../lib/reveal.js-5.1.0/reveal.css" />
    <link rel="stylesheet" href="../lib/reveal.js-5.1.0/theme/solarized.css" />

    <link rel="stylesheet" href="../lib/highlight.js-11.9.0/default.min.css" />
    <script src="../lib/highlight.js-11.9.0/highlight.min.js"></script>

    <link rel="stylesheet" href="../css/styles.css" />
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Assessment State Machine</h2>
          <h4>Managing Assessment State Effectively</h4>

          <br />
          <br />

          <p>Christopher R. Bilger</p>
          <p>January 14th, 2026</p>

          <aside class="notes">
            Welcome everyone! Today I'm excited to share how we transformed our assessment navigation system using a
            state machine architecture. This was a significant project that took about 3 sprints and has fundamentally
            improved how we manage assessment flows at Talkiatry. By the end of this talk, you'll understand the
            problems we faced, how we solved them, and the measurable impact it had on our team and product.
          </aside>
        </section>

        <section>
          <h2>Agenda</h2>

          <br />

          <div style="display: grid; grid-template-columns: 1fr 1fr">
            <div style="align-self: center">
              <ul>
                <li>Problem Statement</li>
                <li>State Machine Overview</li>
                <li>Implementation Details</li>
                <li>Benefits and Outcomes</li>
                <li>Conclusion + Q&A</li>
              </ul>
            </div>

            <div>
              <img data-src="./images/common-player-state-flowchart.png" width="400" />
            </div>
          </div>

          <aside class="notes">
            Here's our agenda. We'll start by understanding the problems we faced with our old navigation system. Then
            I'll explain what a state machine is and how it applies to our assessment flows. Next, we'll dive into the
            implementation details including the four design patterns we used. After that, I'll share concrete metrics
            showing the impact. Finally, we'll have time for Q&A. This should take about 30-35 minutes total.
          </aside>
        </section>

        <section>
          <section>
            <h2>Problem Statement</h2>

            <br />
            <br />

            <ul>
              <li class="fragment">Complex assessment lifecycle with multiple states</li>
              <li class="fragment">Challenges in managing transitions and ensuring consistency</li>
              <li class="fragment">Need for a structured approach to handle state changes</li>
              <li class="fragment">Consistency for analytics and reporting</li>
              <li class="fragment">Difficulty in scaling changes in the assessment</li>
            </ul>

            <aside class="notes">
              Let's start with the problems. Our assessment system is complex - we have intake assessments with ~45
              screens and referral assessments with ~32 screens. Each screen can transition to different screens based
              on user answers, clinical criteria, and business rules. Before the state machine, managing this complexity
              was becoming increasingly difficult. Let me break down the specific challenges we faced.
            </aside>
          </section>

          <section>
            <h2>Complex Assessment Lifecycle</h2>

            <br />

            <ul>
              <li class="fragment">No clear boundary between happy, terminal, and error states</li>
              <li class="fragment">Numerous events triggering state transitions</li>
              <li class="fragment">Interdependencies between states and events</li>
            </ul>

            <aside class="notes">
              The complexity comes from several sources. First, we have normal flow screens, terminal screens like
              "Emergency" for suicide risk, "NoService" for exclusion criteria, and error states. The boundaries between
              these weren't clearly defined in code. Second, transitions are triggered by many different events - user
              clicking next, validation failures, API responses. Third, screens have dependencies - you can't show
              certain screens until prerequisites are met, like guardian consent for minors. This created a web of
              conditional logic scattered throughout the codebase.
            </aside>
          </section>

          <section>
            <h2>Challenges in Managing Transitions</h2>

            <br />
            <br />

            <ul>
              <li class="fragment">Inconsistent handling of state changes</li>
              <li class="fragment">Increased risk of errors and mismanagement</li>
              <li class="fragment">Difficulty in tracking state history and debugging issues</li>
            </ul>

            <aside class="notes">
              These challenges manifested in real problems. Different developers handled state changes differently -
              some in Redux actions, some in component lifecycle methods, some in event handlers. This inconsistency led
              to bugs. For example, we had a bug where age validation worked differently in intake vs referral
              assessments. Debugging was painful because we couldn't easily trace the path a user took through the
              assessment. When something went wrong, it was hard to reproduce and understand why.
            </aside>
          </section>

          <section>
            <h2>Need for a Structured Approach</h2>

            <br />
            <br />

            <ul>
              <li class="fragment">Clear definition of states and transitions</li>
              <li class="fragment">Standardized handling of events</li>
              <li class="fragment">Improved maintainability and scalability</li>
            </ul>

            <aside class="notes">
              What we needed was structure. We needed a single source of truth for what states exist, what transitions
              are valid, and how events trigger those transitions. We needed a standardized way to handle navigation
              that every screen would use consistently. And critically, we needed something maintainable that would
              scale as we added more assessment types and screens. This is where the state machine pattern came in.
            </aside>
          </section>

          <section>
            <h2>Consistency for Analytics & Reporting</h2>

            <br />
            <br />

            <ul>
              <li class="fragment">Standardized logging, monitoring, and reporting</li>
              <li class="fragment">Reliable data for performance metrics</li>
              <li class="fragment">Accurate tracking of assessment outcomes</li>
              <li class="fragment">Enhanced decision-making based on consistent data</li>
            </ul>

            <aside class="notes">
              Another major pain point was analytics. We were manually adding tracking calls to each screen transition.
              This led to inconsistent tracking - some transitions were tracked, others weren't. Event names varied.
              Sometimes we forgot to track terminations. This made it hard to get reliable data on completion rates,
              drop-off points, and user flows. Product and leadership needed this data for decision-making, but we
              couldn't trust it. We needed automatic, consistent tracking that happened without manual intervention.
            </aside>
          </section>

          <section>
            <h2>Difficulty in Scaling Changes</h2>

            <br />
            <br />

            <ul>
              <li class="fragment">Complexity in updating state logic</li>
              <li class="fragment">Increased effort for testing and validation</li>
              <li class="fragment">Risk of introducing inconsistencies during changes</li>
              <li class="fragment">Separation of concerns and modularity challenges</li>
            </ul>

            <aside class="notes">
              Finally, scalability was a huge issue. Adding a new screen meant touching multiple files - the component,
              Redux reducers, actions, analytics calls, and more. This took days and required careful testing across the
              entire flow. When we wanted to add a new validation rule like checking for schizophrenia as an exclusion
              criterion, we had to hunt through the codebase to find where to add it. There was no clear separation
              between navigation logic, validation logic, and side effects like analytics. This tight coupling made
              changes risky and time-consuming.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h2>State Machine Overview</h2>

            <br />
            <br />

            <div style="display: grid; grid-template-columns: 2fr 1fr">
              <div style="align-self: center">
                <ul>
                  <li class="fragment">Definition of a finite state machine (FSM)</li>
                  <li class="fragment">States involved in the assessment process</li>
                  <li class="fragment">Transitions between states based on events</li>
                  <li class="fragment">Responsibilities and actions of a state machine</li>
                </ul>
              </div>

              <div class="fragment">
                <img data-src="./images/me-trying-to-explain-fsm.webp" width="300" />
              </div>
            </div>

            <aside class="notes">
              Now let's talk about the solution - a finite state machine. Some of you may be familiar with state
              machines from computer science classes, but let me give a practical overview of how we applied this
              pattern to our assessment flows. I'll explain what a state machine is, how we model our assessment states,
              and what responsibilities the state machine takes on.
            </aside>
          </section>

          <section>
            <h2>What is a Finite State Machine?</h2>

            <br />

            <ul>
              <li class="fragment">Set of data structures and design patterns</li>
              <li class="fragment">Consists of a finite number of discrete states</li>
              <li class="fragment">Used to model complex behaviors in a structured manner</li>
              <li class="fragment">Transitions between states are triggered by events</li>
              <li class="fragment">Provides clarity and predictability in system behavior</li>
              <li class="fragment">Facilitates easier debugging and maintenance</li>
            </ul>

            <aside class="notes">
              A finite state machine is a computational model made up of a finite number of states. At any given time,
              the machine is in exactly one state. When an event occurs, the machine can transition to a different state
              based on predefined rules. Think of a traffic light - it has three states (red, yellow, green) and
              transitions between them on a timer. Our assessment is more complex, but the same principles apply. The
              key benefits are clarity - you can see all possible states and transitions at a glance - and
              predictability - the same input always produces the same output. This makes debugging much easier.
            </aside>
          </section>

          <section>
            <h2>States in the Assessment Process</h2>

            <br />

            <ul>
              <li class="fragment"><span class="blue">Initial State</span>: Assessment not started</li>
              <li class="fragment"><span class="blue">In Progress</span>: Assessment is being conducted</li>
              <li class="fragment"><span class="blue">Completed</span>: Assessment has been finished</li>
              <li class="fragment">
                <span class="blue">Terminated</span>: Assessment was terminated before completion
              </li>
              <li class="fragment"><span class="blue">Error</span>: An error occurred during the assessment process</li>
            </ul>

            <aside class="notes">
              In our system, we model assessment states at a high level. Initial state is before the user starts. In
              Progress covers all the normal flow screens where they're answering questions. Completed means they
              successfully finished the entire assessment. Terminated covers cases where we had to stop early - like if
              they're a suicide risk and need emergency care, or if they have schizophrenia which we don't treat. Error
              states handle technical failures. But here's the interesting part - we actually model this at two levels.
              At the high level is the document status. At a lower level, each screen is effectively a state in the
              machine.
            </aside>
          </section>

          <section>
            <h2>Screen Transitions Based on Events</h2>

            <br />

            <ul>
              <li class="fragment">
                Root event tells the state machine to go to a new screen based on the current screen and some additional
                context
              </li>
              <li class="fragment">
                Events include user actions (e.g., start, submit, terminate) and system events (e.g., error occurrence)
              </li>
              <li class="fragment">
                Each event triggers a transition from one state to another, updating the assessment status accordingly
              </li>
            </ul>

            <aside class="notes">
              Transitions happen when events occur. The main event is the user clicking "Next" - we call this the root
              navigation event. When this happens, the state machine looks at the current screen, the user's answers,
              their demographics, and other context to determine what screen to show next. For example, if they answer
              "Yes" to active suicidal ideation, we immediately transition to the Emergency screen. If they're under 18
              and not with a guardian, we terminate. The state machine encapsulates all this logic in one place, making
              it easy to understand and modify.
            </aside>
          </section>

          <section>
            <h2>State Machine Navigation Observers</h2>

            <br />

            <table style="font-size: 0.6em">
              <tr>
                <th>Observer</th>
                <th>Purpose</th>
                <th>Trigger</th>
              </tr>

              <tr>
                <td><span class="blue">FreshpaintAnalyticsObserver</span></td>
                <td>Track screen views and terminations</td>
                <td>Every navigation</td>
              </tr>

              <tr>
                <td><span class="blue">DatadogObserver</span></td>
                <td>RUM tracking and performance monitoring</td>
                <td>Every navigation</td>
              </tr>

              <tr>
                <td><span class="blue">ConsoleLogObserver</span></td>
                <td>Development debugging (non-prod only)</td>
                <td>Every navigation</td>
              </tr>

              <tr>
                <td><span class="blue">ReduxPersistorObserver</span></td>
                <td>Control state persistence timing</td>
                <td>Specific screens</td>
              </tr>

              <tr>
                <td><span class="blue">ProgressObserver</span></td>
                <td>Calculate and track assessment progress</td>
                <td>Step screens</td>
              </tr>

              <tr>
                <td><span class="blue">DocumentUpdateObserver</span></td>
                <td>Update database on termination</td>
                <td>Terminations</td>
              </tr>
            </table>

            <aside class="notes">
              One of the most powerful features is the observer pattern. Every time a navigation happens, the state
              machine notifies all registered observers. We have six observers currently. FreshpaintAnalyticsObserver
              automatically tracks every screen view - no more manual tracking calls! DatadogObserver sends metrics to
              our monitoring system for RUM tracking. ConsoleLogObserver helps us debug in development.
              ReduxPersistorObserver controls when we save state to localStorage - we pause during sensitive screens
              like email collection. ProgressObserver calculates how far through the assessment the user is.
              DocumentUpdateObserver updates the database when assessments terminate. The beauty is that adding a new
              observer is trivial - just implement the interface and register it. The navigation logic doesn't need to
              know about any of this.
            </aside>
          </section>

          <section>
            <h2>State Machine Navigation Handlers</h2>

            <br />

            <ul>
              <li class="fragment">Reusable components for verifying context between screen navigation</li>
              <li class="fragment">
                Validate navigation rules and conditions

                <ul>
                  <li class="fragment">Handling invalid navigation attempts gracefully</li>
                  <li class="fragment">
                    Ensuring prerequisites are met before transitioning to a new screen

                    <ul class="fragment">
                      <li class="fragment">Age check</li>
                      <li class="fragment">State check</li>
                      <li class="fragment">Guardian checks</li>
                      <li class="fragment">Schizophrenia check</li>
                      <li class="fragment">Suicide-risk check</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <aside class="notes">
              Handlers are reusable validation components that we chain together. Each handler checks one specific
              condition. AgeValidationHandler verifies the user meets age requirements. StateValidationHandler confirms
              we serve their state. GuardianRequirementHandler ensures minors have guardian consent. Clinical handlers
              check for schizophrenia, recent inpatient stays, and active suicide risk. The brilliant part is these
              handlers are completely reusable - we use the exact same handlers in both intake and referral assessments.
              When we add a new validation rule, we create a new handler and chain it in. This modular approach means
              validation logic is never duplicated and is easy to test in isolation.
            </aside>
          </section>

          <section>
            <h2>State Machine Navigation Strategies</h2>

            <br />

            <ul>
              <li class="fragment">Determine the appropriate navigation path based on current state and events</li>
              <li class="fragment">Implement fallback strategies for unexpected states or events</li>
              <li class="fragment">Optimize navigation for user experience and performance</li>
              <li class="fragment">
                Modular and extensible design such that reusability is high between intake and referral assessments
              </li>
            </ul>

            <aside class="notes">
              Strategies define the navigation logic for each screen. Every screen has a corresponding strategy class
              that implements one method: getNextScreen. This method looks at the context - current answers,
              demographics, etc - and returns which screen to show next. Some strategies are simple - just return the
              next screen. Others are complex and use handler chains for validation. The key is that each strategy is
              isolated and testable. We can test the DemographicsStrategy without touching any other part of the system.
              And again, we share strategies between assessments whenever possible. This modular design makes the
              codebase much more maintainable.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h2>Implementation Details</h2>

            <br />
            <br />

            <div style="display: grid; grid-template-columns: 2fr 1fr">
              <div style="align-self: center">
                <ul>
                  <li class="fragment">Four core design patterns working together</li>
                  <li class="fragment">Type-safe navigation context</li>
                  <li class="fragment">Reusable validation handlers</li>
                  <li class="fragment">Pluggable observer architecture</li>
                </ul>
              </div>

              <div class="fragment">
                <img data-src="./images/turing-machine-always-has-been.jpeg" width="300" />
              </div>
            </div>

            <aside class="notes">
              Now let's dive into the implementation. The architecture is built on four classic design patterns working
              in concert. We use TypeScript throughout for type safety. The navigation context is a builder pattern that
              packages all the data needed for navigation decisions. Handlers use the Chain of Responsibility pattern.
              Observers use the Observer pattern. And strategies use, well, the Strategy pattern. There's also a State
              pattern for managing flow states. Let me show you how these work together. Timing note: This is the most
              technical section, I'll keep it concise.
            </aside>
          </section>

          <section>
            <h2>Architecture Overview</h2>

            <br />

            <p style="font-size: 0.8em">
              <span class="blue">Flow:</span> User Action → Context → State → Strategy → Handlers → Observers → Result
            </p>

            <img data-src="./images/assessment-state-machine-architecture.svg" width="700" />

            <aside class="notes">
              This diagram shows the complete flow. When a user clicks Next, we build a navigation context with all
              relevant data. The state machine checks its current flow state - are we in normal flow, interrupted flow
              for insurance verification, or terminal flow? Based on the flow state and current screen, it selects the
              appropriate strategy. The strategy may use handler chains for validation. If validation passes, the
              strategy returns the next screen. The state machine then notifies all observers before returning the
              result. This happens in milliseconds and is completely transparent to the UI layer. The key insight is
              that each component has a single, clear responsibility.
            </aside>
          </section>

          <section>
            <h2>Core Design Patterns</h2>

            <br />

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; font-size: 0.9em">
              <div class="fragment">
                <h4>
                  <span class="blue">State Pattern</span>
                </h4>

                <p style="font-size: 0.85em">Manages flow behavior (Normal, Interrupted, Terminal)</p>
              </div>

              <div class="fragment">
                <h4>
                  <span class="blue">Strategy Pattern</span>
                </h4>

                <p style="font-size: 0.85em">Screen-specific navigation logic</p>
              </div>

              <div class="fragment">
                <h4>
                  <span class="blue">Chain of Responsibility</span>
                </h4>

                <p style="font-size: 0.85em">Composable validation handlers</p>
              </div>

              <div class="fragment">
                <h4>
                  <span class="blue">Observer Pattern</span>
                </h4>

                <p style="font-size: 0.85em">Decoupled side effects (analytics, logging)</p>
              </div>
            </div>

            <br />

            <p class="fragment" style="font-size: 0.85em">
              <span class="blue">Result:</span> Behavior changes without conditionals, isolated testable components
            </p>

            <aside class="notes">
              Let me quickly define each pattern. State pattern lets the state machine behave differently based on its
              current state - normal flow vs terminal flow behave completely differently. Strategy pattern encapsulates
              each screen's navigation algorithm. Chain of Responsibility links handlers so each can validate and pass
              to the next. Observer pattern decouples side effects from navigation logic. The result is beautiful - we
              have zero long conditional chains. Each component is small, focused, and testable. And behavior changes
              dynamically based on state without any if-else logic. This is the power of good design patterns working
              together.
            </aside>
          </section>

          <section>
            <h2>Pattern Example: Chain of Responsibility</h2>

            <pre>
              <code class="language-typescript">
abstract class BaseNavigationHandler {
  private nextHandler: INavigationHandler | null = null;
  setNext(handler: INavigationHandler) { this.nextHandler = handler; }
  protected next(context) { return this.nextHandler?.handle(context); }
}

class AgeValidationHandler extends BaseNavigationHandler {
  handle(context: NavigationContext) {
    if (isUnder18 && context.isGuardian === false) {
      return this.terminate(Screens.MinorTermination);
    }
    return this.next(context); // Continue chain
  }
}

// Chain handlers together
new AgeValidationHandler()
  .setNext(new StateValidationHandler())
  .setNext(new GuardianRequirementHandler())
  .setNext(new SuicideRiskCheckHandler());
              </code>
            </pre>

            <aside class="notes">
              Here's real code showing Chain of Responsibility. The base class provides the chaining mechanism via
              setNext. Each handler implements the handle method. If validation fails, it returns a termination result.
              If validation passes, it calls this.next to continue the chain. Look at how clean this is - each handler
              is 5-10 lines, does one thing, and is trivial to test. At the bottom, you can see how we chain handlers
              together. This same chain is used in multiple strategies across both assessment types. If we need to add a
              new validation, we just create a new handler and insert it in the chain. No existing code changes. This is
              the Open/Closed Principle in action.
            </aside>
          </section>

          <section>
            <h2>Extensibility: Adding Features</h2>

            <br />

            <div style="text-align: left; font-size: 0.85em">
              <div class="fragment">
                <p><span class="blue">New Screen:</span> Create strategy + register</p>

                <pre style="margin-top: 10px">
                  <code class="language-typescript">registry.set(Screens.NewScreen, new NewScreenStrategy());</code>
                </pre>
              </div>

              <div class="fragment" style="margin-top: 25px">
                <p><span class="blue">New Validation:</span> Create handler + chain</p>

                <pre style="margin-top: 10px">
                  <code class="language-typescript">chain.setNext(new CustomValidationHandler());</code>
                </pre>
              </div>
            </div>

            <aside class="notes">
              The true test of architecture is extensibility. How easy is it to add new features? With our state
              machine, it's incredibly simple. New screen? Create a strategy class and register it. That's it.
              Analytics, progress tracking, persistence - all automatic. New validation rule? Create a handler and chain
              it. Again, observers work automatically. New side effect like sending data to a new analytics service?
              Create an observer and register it. No changes to navigation logic. This is why we went from 2-3 days to
              add a feature down to 3-4 hours. Most of that time is writing tests, not implementation. The architecture
              practically writes itself.
            </aside>
          </section>

          <section>
            <h2>Extensibility: Adding Features (Cont.)</h2>

            <br />

            <div style="text-align: left; font-size: 0.85em">
              <div class="fragment" style="margin-top: 25px">
                <p><span class="blue">New Side Effect:</span> Create observer + register</p>

                <pre style="margin-top: 10px">
                  <code class="language-typescript">stateMachine.addObserver(new CustomObserver());</code>
                </pre>
              </div>

              <p class="fragment" style="margin-top: 50px; text-align: center">
                <span class="blue">Analytics, logging, persistence handled automatically</span>
              </p>
            </div>

            <aside class="notes">
              The true test of architecture is extensibility. How easy is it to add new features? With our state
              machine, it's incredibly simple. New screen? Create a strategy class and register it. That's it.
              Analytics, progress tracking, persistence - all automatic. New validation rule? Create a handler and chain
              it. Again, observers work automatically. New side effect like sending data to a new analytics service?
              Create an observer and register it. No changes to navigation logic. This is why we went from 2-3 days to
              add a feature down to 3-4 hours. Most of that time is writing tests, not implementation. The architecture
              practically writes itself.
            </aside>
          </section>

          <section>
            <h2>Testing Strategy</h2>

            <br />

            <div style="font-size: 0.85em">
              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; text-align: left">
                <div>
                  <h4 class="fragment blue">Unit Testing</h4>

                  <ul class="fragment" style="font-size: 0.9em">
                    <li>Test each pattern in isolation</li>
                    <li>Mock context and dependencies</li>
                    <li>Fast, deterministic</li>
                  </ul>
                </div>

                <div>
                  <h4 class="fragment blue">Integration Testing</h4>

                  <ul class="fragment" style="font-size: 0.9em">
                    <li>Test full navigation flows</li>
                    <li>Verify observer notifications</li>
                    <li>End-to-end scenarios</li>
                  </ul>
                </div>
              </div>

              <div class="fragment" style="margin-top: 40px">
                <p style="text-align: center">
                  <span class="blue">Result:</span> ~98% test coverage of all state machine code
                </p>
              </div>
            </div>

            <aside class="notes">
              Testing was a huge win. Every component is testable in isolation. We can unit test a handler without
              loading the entire app. Mock the context, call handle, verify the result. Fast and deterministic.
              Strategies are just as easy. We went from ~45% test coverage to ~98%. And the tests run significantly
              faster because they're isolated. For integration tests, we can test entire flows by chaining components
              together. We verify that observers get called correctly. We test edge cases like interrupted flows and
              terminations. The architecture itself makes testing easier and more reliable. When tests are this easy to
              write, developers actually write them.
            </aside>
          </section>

          <section>
            <h2>Key Takeaways: Implementation</h2>

            <br />

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; font-size: 0.85em; text-align: left">
              <div>
                <h4 class="blue">Pattern Synergy</h4>

                <ul class="fragment">
                  <li>State manages flow</li>
                  <li>Strategy handles screens</li>
                  <li>Chain validates rules</li>
                  <li>Observer tracks effects</li>
                </ul>
              </div>

              <div>
                <h4 class="blue">Development Benefits</h4>

                <ul class="fragment">
                  <li>Add features in hours</li>
                  <li>Compose validations</li>
                  <li>Plug in observers</li>
                  <li>Test in isolation</li>
                </ul>
              </div>
            </div>

            <p class="fragment" style="margin-top: 40px; font-size: 0.95em; text-align: center">
              <span class="blue">A maintainable, extensible, testable architecture</span>
            </p>

            <aside class="notes">
              Let me summarize the implementation. Four patterns work together - State manages flow behavior, Strategy
              handles screen logic, Chain validates rules, and Observer handles side effects. Each pattern reinforces
              the others. The development benefits are tangible - features that took days now take hours, validations
              are composable building blocks, observers plug in without touching existing code, and everything is
              testable. This isn't theoretical - it's how we work every day now. The architecture is maintainable,
              extensible, and testable. That's the whole point. Let's now look at the concrete benefits we measured.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h2>Benefits and Outcomes</h2>

            <br />

            <div style="text-align: left; font-size: 0.9em">
              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px">
                <div>
                  <h4 class="fragment" style="color: #e53935">Before</h4>

                  <ul class="fragment" style="font-size: 0.9em">
                    <li>Scattered navigation logic</li>
                    <li>Inconsistent validation</li>
                    <li>Manual analytics</li>
                    <li>Hard to test</li>
                  </ul>
                </div>

                <div>
                  <h4 class="fragment" style="color: #7cb342">After</h4>

                  <ul class="fragment" style="font-size: 0.9em">
                    <li>Centralized state machine</li>
                    <li>Reusable handlers</li>
                    <li>Automatic tracking</li>
                    <li>Isolated tests</li>
                  </ul>
                </div>
              </div>
            </div>

            <aside class="notes">
              Now for the good stuff - the results. Let's start with a before and after comparison. Before, navigation
              logic was scattered across components, reducers, and actions. Validation was inconsistent - implemented
              differently in different places. Analytics required manual tracking calls that were often forgotten.
              Testing was difficult because of tight coupling. After implementing the state machine, we have a single
              centralized system. Handlers are reused everywhere. Analytics happens automatically. Tests are isolated
              and fast. This is night and day. Let me show you some specific metrics.
            </aside>
          </section>

          <section>
            <h2>Real-World Impact: Metrics</h2>

            <br />

            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; font-size: 0.75em">
              <div class="fragment" style="background: #e3f2fd; padding: 20px; border-radius: 10px">
                <h3 style="color: #1976d2; margin: 0">Test Coverage</h3>

                <p style="font-size: 1.5em; margin: 10px 0; color: #1976d2">~98% overall</p>
                <p style="margin: 0">State machine unit test coverage</p>
              </div>

              <div class="fragment" style="background: #f3e5f5; padding: 20px; border-radius: 10px">
                <h3 style="color: #7b1fa2; margin: 0">Test Coverage</h3>

                <p style="font-size: 1.5em; margin: 10px 0; color: #7b1fa2">+42% assessment</p>
                <p style="margin: 0">Unit test coverage increase</p>
              </div>

              <div class="fragment" style="background: #e8f5e9; padding: 20px; border-radius: 10px">
                <h3 style="color: #388e3c; margin: 0">Code Reuse</h3>

                <p style="font-size: 1.5em; margin: 10px 0; color: #388e3c">~75%</p>
                <p style="margin: 0">Shared between assessments</p>
              </div>

              <div class="fragment" style="background: #f9ead3; padding: 20px; border-radius: 10px">
                <h3 style="color: #f57c00; margin: 0">Navigation Bugs</h3>

                <p style="font-size: 1.5em; margin: 10px 0; color: #f57c00">Zero</p>
                <p style="margin: 0">Since implementation</p>
              </div>
            </div>

            <aside class="notes">
              Here are the headline numbers. Development speed improved ~80% - features that took 2-3 days now take 3-4
              hours. Test coverage jumped from 45% to 87% - a 42 percentage point increase. We achieve 75% code reuse
              between intake and referral assessments - handlers, observers, and many strategies are shared. And most
              impressively, we've had ZERO navigation bugs since implementation. Not one bug related to state
              transitions or navigation logic in production. These aren't theoretical improvements - these are measured
              before and after metrics. Let me give you a concrete example.
            </aside>
          </section>

          <section>
            <h2>Example: Faster Development</h2>

            <br />

            <div style="font-size: 0.85em; text-align: center">
              <p class="fragment"><span class="blue">Task:</span> Add guardian consent flow for minors</p>

              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px; text-align: left">
                <div class="fragment">
                  <h5 style="color: #e53935; text-align: center">Before</h5>

                  <ul style="font-size: 0.9em">
                    <li>Update multiple files</li>
                    <li>Add conditionals</li>
                    <li>Manual analytics</li>
                    <li>Test all flows</li>
                    <li><strong>~2-3 days</strong></li>
                  </ul>
                </div>

                <div class="fragment">
                  <h5 style="color: #7cb342; text-align: center">After</h5>

                  <ul style="font-size: 0.9em">
                    <li>Create handler</li>
                    <li>Register strategy</li>
                    <li>Automatic tracking</li>
                    <li>Test in isolation</li>
                    <li><strong>~3-4 hours</strong></li>
                  </ul>
                </div>
              </div>

              <p class="fragment" style="margin-top: 30px">
                <span class="blue" style="font-size: 1.1em">~80% reduction in development time</span>
              </p>
            </div>

            <aside class="notes">
              Here's a real example. We needed to add a guardian consent flow for minors - when a user under 18 signs
              up, we need to show a consent screen for their guardian. Before the state machine, this would have taken
              2-3 days. We'd need to update multiple components, add conditionals in Redux actions, manually add
              analytics tracking, and test across the entire flow to make sure we didn't break anything. After the state
              machine, it took 3-4 hours. We created a GuardianRequirementHandler, chained it into the demographics
              strategy, and that's it. Analytics happened automatically. Tests were isolated to just the handler. This
              is the ~80% improvement - same feature, 6-8 times faster to implement. And more reliable because we can't
              forget tracking or introduce bugs in unrelated code.
            </aside>
          </section>

          <section>
            <h2>Lessons Learned</h2>

            <br />

            <div style="text-align: left; font-size: 0.85em">
              <div class="fragment">
                <h4 class="blue">1. Pattern Selection Matters</h4>

                <p style="margin: 5px 0 15px 20px">Four patterns working together created powerful synergy</p>
              </div>

              <div class="fragment">
                <h4 class="blue">2. Incremental Migration Works</h4>

                <p style="margin: 5px 0 15px 20px">Migrated screen-by-screen without disruption</p>
              </div>

              <div class="fragment">
                <h4 class="blue">3. Testing Investment Pays Off</h4>

                <p style="margin: 5px 0 15px 20px">Comprehensive tests enabled confident refactoring</p>
              </div>

              <div class="fragment">
                <h4 class="blue">4. Developer Buy-In Essential</h4>

                <p style="margin: 5px 0 15px 20px">Architecture reviews and pair programming accelerated adoption</p>
              </div>
            </div>

            <aside class="notes">
              Let me share four key lessons. First, pattern selection matters. We could have used different patterns,
              but State + Strategy + Chain + Observer created powerful synergy. Each pattern reinforced the others.
              Second, incremental migration was critical. We didn't rewrite everything at once. We migrated screen by
              screen over 3 sprints, with the old and new systems running in parallel. This reduced risk and let us
              validate the approach early. Third, investing in tests upfront paid massive dividends. Writing tests as we
              migrated gave us confidence to refactor aggressively. Fourth, developer buy-in was essential. We did
              architecture reviews, pair programming sessions, and documentation. Getting the team comfortable with the
              patterns accelerated adoption and ensured consistency.
            </aside>
          </section>

          <section>
            <h2>Future Enhancements</h2>

            <br />

            <div style="text-align: left; font-size: 0.85em">
              <ul>
                <li class="fragment">
                  <span class="blue">Visual state machine editor</span> - Generate code from diagrams
                </li>

                <li class="fragment">
                  <span class="blue">Enhanced error recovery</span> - Automatic retry and resume
                </li>
              </ul>
            </div>

            <aside class="notes">
              Looking forward, we have some exciting enhancements planned. First, a visual state machine editor where
              non-technical stakeholders could see and modify flows, generating code automatically. Second, enhanced
              error recovery - if an API call fails, automatically retry or resume from the last known good state.
              Third, A/B testing integration where we could have strategy variants based on LaunchDarkly experiments and
              measure which navigation flows have better completion rates. Fourth, ML-powered insights - analyze
              navigation patterns to predict which users are at risk of dropping off and optimize the flow to reduce
              abandonment. The state machine architecture makes all of these much easier to implement than they would
              have been before.
            </aside>
          </section>
        </section>

        <section>
          <section>
            <h2>Conclusion + Q&A</h2>

            <br />

            <div style="display: grid; grid-template-columns: 1fr 1fr">
              <div style="align-self: center">
                <ul>
                  <li>Problem Statement</li>
                  <li>State Machine Overview</li>
                  <li>Implementation Details</li>
                  <li>Benefits and Outcomes</li>
                  <li>Q&A</li>
                </ul>
              </div>

              <div>
                <img data-src="./images/hungry-full-state-machine.jpeg" width="300" />
              </div>
            </div>

            <aside class="notes">
              Let me wrap up. We started with complex, scattered navigation logic that was hard to maintain and scale.
              We implemented a state machine using four design patterns - State, Strategy, Chain of Responsibility, and
              Observer. The results were dramatic - ~80% faster development, zero bugs, 42% more test coverage, and 75%
              code reuse. The key insight is that good architecture isn't theoretical - it has measurable business
              impact. We ship features faster with higher quality. Now I'd love to take your questions. Some common ones
              I expect: How long did migration take? What was the biggest challenge? Would I do anything differently?
              How do you handle edge cases? What about performance? Fire away!
            </aside>
          </section>

          <section>
            <h2>Additional Resources on Finite State Machines</h2>

            <br />
            <br />

            <ol>
              <li>
                <a
                  href="https://ocw.mit.edu/courses/6-004-computation-structures-spring-2017/pages/c6/"
                  target="_blank"
                >
                  MIT OpenCourseWare
                </a>
              </li>

              <li>
                <a href="https://refactoring.guru/design-patterns" target="_blank">Design Patterns</a>
              </li>
            </ol>

            <aside class="notes">
              For those interested in learning more about state machines and design patterns, I highly recommend MIT's
              OpenCourseWare on computation structures - it has great content on finite state machines. And Refactoring
              Guru is an excellent resource for design patterns with visual diagrams and code examples in multiple
              languages. Both are free and comprehensive.
            </aside>
          </section>

          <section>
            <h2>Powered By</h2>

            <br />
            <br />

            <ol>
              <li>A single, static webpage.</li>
              <li>
                <a href="https://revealjs.com/" target="_blank">reveal.js</a>
              </li>
              <li>
                <a href="https://highlightjs.org/" target="_blank">highlight.js</a>
              </li>
            </ol>

            <aside class="notes">
              And just for fun - this entire presentation is a single HTML file using reveal.js for the slide framework
              and highlight.js for syntax highlighting. No build process, no dependencies. Just open it in a browser.
              Simple tools can be powerful. Thanks everyone!
            </aside>
          </section>
        </section>
      </div>
    </div>

    <script type="module" src="../js/initialize-reveal-js-and-highlight-js.js"></script>
  </body>
</html>
